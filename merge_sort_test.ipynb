{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67daad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%maven junit:junit:4.13.2\n",
    "%load djl_imports\n",
    "import ai.djl.training.loss.L2Loss;\n",
    "\n",
    "NDManager manager = NDManager.newBaseManager();\n",
    "    \n",
    "public void print(Object o) {\n",
    "    System.out.println(o);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92b4b7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "public NDArray add10(NDArray parameter) {\n",
    "    return parameter.add(10);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e8890ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 Loss: 4050.0\n",
      "Iteration: 1 Loss: 1012.5\n",
      "Iteration: 2 Loss: 253.125\n",
      "Iteration: 3 Loss: 63.28125\n",
      "Iteration: 4 Loss: 15.8203125\n",
      "Iteration: 5 Loss: 3.9550781\n",
      "Iteration: 6 Loss: 0.98876953\n",
      "Iteration: 7 Loss: 0.24719238\n",
      "Iteration: 8 Loss: 0.061798096\n",
      "Iteration: 9 Loss: 0.015449524\n",
      "Learned parameter: 89.91211\n"
     ]
    }
   ],
   "source": [
    "try (NDManager manager = NDManager.newBaseManager()) {\n",
    "    // Initialize a learnable parameter and expected value\n",
    "    NDArray parameter = manager.create(0.0f); // Start with an initial guess\n",
    "    NDArray expected = manager.create(new float[]{100.0f});\n",
    "    // Set gradients for the input param\n",
    "    parameter.setRequiresGradient(true);\n",
    "\n",
    "    // Training loop\n",
    "    float learningRate = 0.5f;\n",
    "    for (int i = 0; i < 10; i++) {\n",
    "        // Create a new gradient collector - these need to made each training iteration\n",
    "        try (GradientCollector gc = Engine.getInstance().newGradientCollector()) {\n",
    "            // \n",
    "            NDArray result = add10(parameter);\n",
    "            Loss lossFunction = Loss.l2Loss();\n",
    "            NDArray loss = lossFunction.evaluate(new NDList(expected), new NDList(result));\n",
    "\n",
    "            gc.backward(loss); // Calculate gradients\n",
    "            print(\"Iteration: \" + i + \" Loss: \" + loss.getFloat());\n",
    "\n",
    "        }\n",
    "        // Update the parameter (gradient descent)\n",
    "        parameter.subi(parameter.getGradient().mul(learningRate)); \n",
    "        // Clear gradients otherwise memory creeps\n",
    "        parameter.getGradient().close();\n",
    "    }\n",
    "\n",
    "    // We need the parameter to be close to `90` in order \n",
    "    // for the result to match the expected value of `100`\n",
    "    print(\"Learned parameter: \" + parameter.getFloat());\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f32e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "java",
   "file_extension": ".jshell",
   "mimetype": "text/x-java-source",
   "name": "Java",
   "pygments_lexer": "java",
   "version": "11.0.16+8-post-Ubuntu-0ubuntu120.04"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
